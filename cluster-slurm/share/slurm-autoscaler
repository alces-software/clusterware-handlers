#!/bin/bash
################################################################################
##
## Alces Clusterware - Handler support script
## Copyright (C) 2016 Stephen F. Norledge and Alces Software Ltd.
##
################################################################################
setup() {
    local a xdg_config
    IFS=: read -a xdg_config <<< "${XDG_CONFIG_HOME:-$HOME/.config}:${XDG_CONFIG_DIRS:-/etc/xdg}"
    for a in "${xdg_config[@]}"; do
        if [ -e "${a}"/clusterware/config.vars.sh ]; then
            source "${a}"/clusterware/config.vars.sh
            break
        fi
    done
    if [ -z "${cw_ROOT}" ]; then
        echo "$0: unable to locate clusterware configuration"
        exit 1
    fi
    kernel_load
}

# TODO: Can move to external module.
_collate_group_dimensions() {
    local dims groups g

    groups=()
    _gather_groups() {
        while [ "$1" != "--" ]; do shift; done
        groups+=($(member_find_tag "aws_group" "$5"))
    }
    member_each _gather_groups
    groups=($(printf "%s\n" "${groups[@]}" | sort -u | tr '\n' ' '))
    log "[autoscaler:groups] Found groups: ${groups[*]}"

    dims=()
    for g in "${groups[@]}"; do
        dims+=("$("${_JO}" Name=AutoScalingGroupName Value=${g})")
    done

    "${_JO}" -a "${dims[@]}"
}

_set_job_metrics() {
    # for all known scaling groups, set job metrics
    local dims tmpfile

    dims="$(_collate_group_dimensions)"
    tmpfile="$(mktemp /tmp/slurm-autoscaler.XXXXXXXX)"

    # TODO generalize passing state obtaining func?
    squeue -O state | tail -n+2 | sort | uniq -c | (
        t=0
        metrics=()
        seen_states=()
        # TODO generalize naming
        if [ -f "${cw_ROOT}"/var/lib/db/cluster-slurm/statetab ]; then
            known_states=($(cat "${cw_ROOT}"/var/lib/db/cluster-slurm/statetab))
        fi
        while read c s; do
            log "[autoscaler:metrics] Found ${c} jobs in state '${s}'"
            t=$(($t+$c))
            metrics+=($("${_JO}" MetricName=JobStatus_$s Value=$c Unit=Count "Dimensions=${dims}"))
            seen_states+=($s)
        done

        for state in "${known_states[@]}"; do
            if [[ " ${seen_states[*]} " != *" ${state} "* ]]; then
                log "[autoscaler:metrics] No jobs in previously seen state '${state}'"
                metrics+=($("${_JO}" MetricName=JobStatus_$state Value=0 Unit=Count "Dimensions=${dims}"))
            fi
        done

        known_states+=(${seen_states[@]})
        log "[autoscaler:metrics] Update known states table"
        # TODO: generalize naming
        mkdir -p "${cw_ROOT}"/var/lib/db/cluster-slurm
        printf "%s\n" "${known_states[@]}" | sort -u > "${cw_ROOT}"/var/lib/db/cluster-slurm/statetab

        log "[autoscaler:metrics] Found ${t} jobs total"
        metrics+=($("${_JO}" MetricName=JobTotal Value=$t Unit=Count "Dimensions=${dims}"))
        "${_JO}" -a "${metrics[@]}" > "${tmpfile}"
    )

    log "[autoscaler:metrics] Sending metric data to cloudwatch"
    cat "${tmpfile}" | _log_blob "metrics"

    # TODO: Change ALCES-SGE based on decision - we need to either have
    # different templates for SGE and Slurm, with different autoscaling
    # namespace and MetricName to watch for, or maybe could have one template
    # with different auto scaling groups for different schedulers and scheduler
    # chosen as parameter.
    result=$("${_AWS}" --region "${cw_INSTANCE_aws_region}" \
                       cloudwatch put-metric-data \
                       --namespace "ALCES-SLURM" \
                       --metric-data file://${tmpfile} 2>&1)
    if echo "${result}" | grep -q AccessDenied; then
        log "[autoscaler:metrics:aws] Access denied for PutMetricData; disabling."
        echo "disabled" > "${cw_ROOT}"/var/lib/db/cluster-sge/disable-set_job_metrics
    else
        echo "${result}" | _log_blob "metrics:aws"
    fi

    rm -f "${tmpfile}"
}

_empty_nodes() {
  sinfo --states=idle --Format=nodehost | tail -n+2
}

# TODO: Can move to external module.
_unexhausted_nodes() {
    local node nodelist ctime
    nodelist="$*"
    for node in ${nodelist}; do
        ctime=$(member_get_member_tag "${node}" "aws_ctime")
        if [ "${ctime}" ]; then
            ruby_run <<RUBY
require 'time'
delta = Time.now - Time.parse('${ctime}')
print "${node} " if delta % 3600 <= 3180
RUBY
        else
            # unable to find ctime, assume unexhausted
            echo -n "${node} "
        fi
    done
}

_log_blob() {
    local prefix
    prefix="$1"
    log_blob "${cw_CLUSTER_SLURM_log}" "autoscaler:${prefix}"
}

# TODO: Can move to external module?
_shoot_node() {
    local node instanceid group
    node="$1"
    group="$2"
    instanceid=$(member_get_member_tag "${node}" "aws_instanceid")

    # scale in the capacity
    log "[autoscaler:shoot] Shooting node ${instanceid} in group ${group}"
    "${_AWS}" --region "${cw_INSTANCE_aws_region}" \
      autoscaling terminate-instance-in-auto-scaling-group \
      --instance-id ${instanceid} \
      --should-decrement-desired-capacity 2>&1 | _log_blob "autoscaling:aws"
}

_scale_in() {
    # determine which nodes are viable to reap
    # algorithm is:
    #  - nodes that are currently empty
    #  - nodes that are within last 10 minutes of current hour
    #    (i.e. those that may incur additional cost between now and
    #    the next run)
    empty_nodes="$(_empty_nodes)"
    empty_nodes=($(echo "${empty_nodes}" | tr '\n' ' '))
    log "[autoscaler:scale-in] Found empty nodes: ${empty_nodes[*]}"

    for node in "${empty_nodes[@]}"; do
        group=$(member_get_member_tag "${node}" "aws_group")
        if [ "${group}" ]; then
            log "[autoscaler:scale-in] Disabling queues on ${node} (in group: ${group})"
            # for each empty node, disable the queues while we're deciding
            # whether we're going to shoot them or not.
            scontrol update NodeName=${node} State=DRAIN Reason=autoscaling
        fi
    done

    unexhausted_nodes=" $(_unexhausted_nodes ${empty_nodes[@]}) "
    for node in "${empty_nodes[@]}"; do
        group=$(member_get_member_tag "${node}" "aws_group")
        if [ "${group}" ]; then
            if [[ "$unexhausted_nodes" == *" $node "* ]]; then
                # this node is not exhausted, reenable the queue
                log "[autoscaler:scale-in] Enabling queues on ${node} (not exhausted)"
                scontrol update NodeName=${node} State=RESUME
            else
                log "[autoscaler:scale-in] Attempting to shoot node: ${node}"
                if ! _shoot_node "${node}" "${group}"; then
                    log "[autoscaler:scale-in] Unable to shoot node, re-enabling queues"
                    scontrol update NodeName=${node} State=RESUME
                fi
            fi
        fi
    done
}

main() {
    files_load_config instance-aws config/cluster
    files_load_config cluster-slurm

    log_set_default "${cw_CLUSTER_SLURM_log}"
    if [ ! -f "${cw_ROOT}"/var/lib/db/cluster-slurm/disable-set_job_metrics ]; then
        log "[autoscaler] Performing job metric scan"
        _set_job_metrics
    else
        log "[autoscaler] Skipping metric scan (currently disabled)"
    fi

    if [ "${cw_INSTANCE_aws_autoscaling}" == "enabled" ]; then
        log "[autoscaler] Performing scale-in check"
        _scale_in
    else
        log "[autoscaler] Autoscaling is disabled"
    fi
}

. /etc/profile.d/alces-clusterware.sh
setup

module purge
module use "${cw_ROOT}"/etc/modules
module load services/slurm

require member
require files
require ruby
require handler
require network
require log

PATH=$PATH:/sbin

_JO="${cw_ROOT}"/opt/jo/bin/jo
_JQ="${cw_ROOT}"/opt/jq/bin/jq
_AWS="${cw_ROOT}"/opt/aws/bin/aws

main "$@"
