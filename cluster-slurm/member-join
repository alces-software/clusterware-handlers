#!/bin/bash
################################################################################
##
## Alces Clusterware - Handler hook
## Copyright (C) 2015-2016 Stephen F. Norledge and Alces Software Ltd.
##
################################################################################
setup() {
    local a xdg_config
    IFS=: read -a xdg_config <<< "${XDG_CONFIG_HOME:-$HOME/.config}:${XDG_CONFIG_DIRS:-/etc/xdg}"
    for a in "${xdg_config[@]}"; do
        if [ -e "${a}"/clusterware/config.vars.sh ]; then
            source "${a}"/clusterware/config.vars.sh
            break
        fi
    done
    if [ -z "${cw_ROOT}" ]; then
        echo "$0: unable to locate clusterware configuration"
        exit 1
    fi
    kernel_load
}

main() {
    local hook_dir tags tag tuple key value vmem_gb slots hook_dir \
    scheduler_roles instanceid group control_node_iptables_rule \
    compute_node_iptables_rule
    files_load_config instance config/cluster

    eval "$(member_parse)"
    slurm_log "Member data parsed: ${cw_MEMBER_name} (${cw_MEMBER_ip}) -- ${cw_MEMBER_tags}"
    "${cw_ROOT}"/libexec/share/update-etc-hosts "${cw_MEMBER_name}" "${cw_MEMBER_ip}"

    slurm_log "Checking roles for new member (${cw_MEMBER_name})"
    IFS=',' read -a tags <<< "${cw_MEMBER_tags}"
    for tag in "${tags[@]}"; do
        IFS='=' read -a tuple <<< "${tag}"
        slurm_log "Found tuple: ${tag}, key: ${tuple[0]}, value: ${tuple[1]}"
        key=${tuple[0]}
        value=${tuple[1]}
        if [ "$key" == "scheduler_roles" ]; then
            scheduler_roles="${value}"
            # TODO: Do we need any of below?
        # elif [ "$key" == "ram_gb" ]; then
        #     vmem_gb="${value}"
        # elif [ "$key" == "slots" ]; then
        #     slots="${value}"
        # elif [ "$key" == "aws_instanceid" ]; then
        #     instanceid="${value}"
        # elif [ "$key" == "aws_group" ]; then
        #     group="${value}"
        fi
    done

    # Update this node's slurm.conf with new node.
    hook_dir=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
    if [[ "${scheduler_roles}" == *":master:"* ]]; then
        slurm_log "${cw_MEMBER_name} is Slurm control node"
        "${hook_dir}/share/slurm-set-control-node.sh" "${cw_MEMBER_name}"
    fi
    if [[ "${scheduler_roles}" == *":compute:"* ]]; then
        slurm_log "${cw_MEMBER_name} is Slurm compute node"
        ruby_exec "${hook_dir}/share/slurm-modify-compute-nodes.rb" join "${cw_MEMBER_name}"
    fi

    # From slurm.conf docs: "Changes in node configuration (e.g. adding nodes,
    # changing their processor count, etc.) require restarting both the
    # slurmctld daemon and the slurmd daemons" - so do this if needed based on
    # this node's role(s).
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
        distro_restart_service clusterware-slurm-slurmctld
    fi
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":compute:"* ]]; then
        distro_restart_service clusterware-slurm-slurmd
    fi

    # Add Slurm iptables rules.
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
        control_node_iptables_rule="$(slurm_control_node_iptables_rule ${cw_MEMBER_ip})"
        slurm_log "control: ${control_node_iptables_rule}"
        if ! iptables -C ${control_node_iptables_rule} &>/dev/null; then
            slurm_log "Adding iptables rule: ${control_node_iptables_rule}"
            iptables -I ${control_node_iptables_rule}
        else
            slurm_log "iptables rule already exists: ${control_node_iptables_rule}"
        fi
    fi
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":compute:"* && "${scheduler_roles}" == *":master:"* ]]; then
        compute_node_iptables_rule="$(slurm_compute_node_iptables_rule ${cw_MEMBER_ip})"
        slurm_log "compute: ${compute_node_iptables_rule}"
        if ! iptables -C ${compute_node_iptables_rule} &>/dev/null; then
            echo "Adding iptables rule: ${compute_node_iptables_rule}"
            slurm_log "Adding iptables rule: ${compute_node_iptables_rule}"
            iptables -I ${compute_node_iptables_rule}
        else
            slurm_log "iptables rule already exists: ${compute_node_iptables_rule}"
        fi
    fi
}

setup
require distro
require member
require handler
require log
require files
require slurm

_AWS="${cw_ROOT}"/opt/aws/bin/aws

if files_load_config --optional cluster-slurm; then
    handler_tee main "$@"
fi

