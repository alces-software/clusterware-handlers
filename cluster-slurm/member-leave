#!/bin/bash
################################################################################
##
## Alces Clusterware - Handler hook
## Copyright (C) 2016 Stephen F. Norledge and Alces Software Ltd.
##
################################################################################
setup() {
    local a xdg_config
    IFS=: read -a xdg_config <<< "${XDG_CONFIG_HOME:-$HOME/.config}:${XDG_CONFIG_DIRS:-/etc/xdg}"
    for a in "${xdg_config[@]}"; do
        if [ -e "${a}"/clusterware/config.vars.sh ]; then
            source "${a}"/clusterware/config.vars.sh
            break
        fi
    done
    if [ -z "${cw_ROOT}" ]; then
        echo "$0: unable to locate clusterware configuration"
        exit 1
    fi
    kernel_load
}

main() {
    local tags tag tuple key value hook_dir intf scheduler_roles
    files_load_config instance config/cluster

    # TODO: Add correct guards like this in
    # . "${cw_ROOT}"/etc/clusterware.rc
    # case "${cw_VERSION:-1.0.0}" in
    #   1.[012].*)
	  # return
    #   ;;
    # esac

    eval "$(member_parse)"
    slurm_log "Member data parsed: ${cw_MEMBER_name} (${cw_MEMBER_ip}) -- ${cw_MEMBER_tags}"
    "${cw_ROOT}"/libexec/share/prune-etc-hosts "${cw_MEMBER_name}" "${cw_MEMBER_ip}"

    slurm_log "Checking roles for left member (${cw_MEMBER_name})"
    IFS=',' read -a tags <<< "${cw_MEMBER_tags}"
    for tag in "${tags[@]}"; do
        IFS='=' read -a tuple <<< "${tag}"
        slurm_log "Found tuple: ${tag}, key: ${tuple[0]}, value: ${tuple[1]}"
        key=${tuple[0]}
        value=${tuple[1]}
        if [ "$key" == "scheduler_roles" ]; then
            scheduler_roles="${value}"
        fi
    done

    # Update this node's slurm.conf with left node.
    hook_dir=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
    if [[ "${scheduler_roles}" == *":master:"* ]]; then
        slurm_log "${cw_MEMBER_name} is Slurm control node"
    fi
    if [[ "${scheduler_roles}" == *":compute:"* ]]; then
        slurm_log "${cw_MEMBER_name} is Slurm compute node"
        ruby_exec "${hook_dir}/share/slurm-modify-compute-nodes.rb" leave "${cw_MEMBER_name}"
    fi

    # From slurm.conf docs: "Changes in node configuration (e.g. adding nodes,
    # changing their processor count, etc.) require restarting both the
    # slurmctld daemon and the slurmd daemons" - so do this if needed based on
    # this node's role(s).
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
        distro_restart_service clusterware-slurm-slurmctld
    fi
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":compute:"* ]]; then
        distro_restart_service clusterware-slurm-slurmd
    fi

    # Remove Slurm iptables rules.
    intf="$(network_get_route_iface ${cw_MEMBER_ip})"
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
        if iptables -C INPUT -i "${intf}" -s "${cw_MEMBER_ip}" -p tcp -j ACCEPT &>/dev/null; then
            slurm_log "Removing iptables rule: -I INPUT -i ${intf} -s ${cw_MEMBER_ip} -p tcp -j ACCEPT"
            iptables -D INPUT -i "${intf}" -s "${cw_MEMBER_ip}" -p tcp -j ACCEPT
        else
            slurm_log "iptables rule not present: -I INPUT -i ${intf} -s ${cw_MEMBER_ip} -p tcp -j ACCEPT"
        fi
    fi
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":compute:"* && "${scheduler_roles}" == *":master:"* ]]; then
        if iptables -C INPUT -i "${intf}" -s "${cw_MEMBER_ip}" -p tcp --dport 6818 -j ACCEPT &>/dev/null; then
            slurm_log "Adding iptables rule: -I INPUT -i ${intf} -s ${cw_MEMBER_ip} -p tcp --dport 6818 -j ACCEPT"
            iptables -D INPUT -i "${intf}" -s "${cw_MEMBER_ip}" -p tcp --dport 6818 -j ACCEPT
        else
            slurm_log "iptables rule not present: -I INPUT -i ${intf} -s ${cw_MEMBER_ip} -p tcp --dport 6818 -j ACCEPT"
        fi
    fi
}

setup
require member
require distro
require network
require handler
require log
require slurm

if files_load_config --optional cluster-slurm; then
    if [ "$cw_CLUSTER_SLURM_cleanup" == "true" ]; then
	handler_tee main "$@"
    fi
fi
