#!/bin/bash
################################################################################
##
## Alces Clusterware - Handler hook
## Copyright (C) 2016 Stephen F. Norledge and Alces Software Ltd.
##
################################################################################
setup() {
    local a xdg_config
    IFS=: read -a xdg_config <<< "${XDG_CONFIG_HOME:-$HOME/.config}:${XDG_CONFIG_DIRS:-/etc/xdg}"
    for a in "${xdg_config[@]}"; do
        if [ -e "${a}"/clusterware/config.rc ]; then
            source "${a}"/clusterware/config.rc
            break
        fi
    done
    if [ -z "${cw_ROOT}" ]; then
        echo "$0: unable to locate clusterware configuration"
        exit 1
    fi
    kernel_load
}

_configure_slurm() {
    local slurm_system_dirs slurm_user munge_path munge_key_dir munge_key munge_user hook_dir hostname

    echo "Configuring Slurm"

    # Create needed system dirs owned by Slurm user.
    slurm_system_dirs=(/var/{log,run,spool}/slurm)
    slurm_user='slurm'
    mkdir -p "${slurm_system_dirs[@]}"
    chown -R "${slurm_user}:${slurm_user}" "${slurm_system_dirs[@]}"

    # Create MUNGE key, required for authentication between nodes - must be the
    # same on all nodes so create by hashing the cluster secret. Note: Create
    # in this hook as need auth.rc to be created which is not done until
    # clusterable preconfigure hook.
    munge_path="$(echo ~munge)"
    munge_key_dir="${munge_path}/etc/munge"
    munge_key="${munge_key_dir}/munge.key"
    munge_user='munge'
    source "${cw_ROOT}/etc/config/cluster/auth.rc"
    echo -n "${cw_CLUSTER_auth_token}" | sha512sum | cut -d' ' -f1 > "${munge_key}"
    chmod 400 "${munge_key}"
    chown -R "${munge_user}:${munge_user}" "${munge_key_dir}"

    # Enable service components based on node's role(s).
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
        echo "Instance is Slurm control node"
        "${_ALCES}" service enable slurm/slurmctld
    fi
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":compute:"* ]]; then
        echo "Instance is Slurm compute node"
        "${_ALCES}" service enable slurm/slurmd
        ruby_run <<RUBY
require 'json'

config = {"tags" => {}}
config["tags"]["slots"] = $(grep -c '^processor\s*: [0-9]*$' /proc/cpuinfo).to_s
ram_kb = $(grep 'MemTotal' /proc/meminfo | awk '{print $2};')
ram_gb = (ram_kb / 1_048_576)
config["tags"]["ram_gb"] = ram_gb.to_s
File.write('${cw_ROOT}/etc/serf/tags-cluster-node-resources.json', config.to_json)
RUBY
    fi
}

main() {
    local handler_config_dir="${cw_ROOT}/etc/config/cluster-slurm"

    # Bail out if we're already configured (this is a reboot).
    if [ -d "${handler_config_dir}" ]; then
        exit 0
    fi

    mkdir -p "${handler_config_dir}"

    files_load_config instance config/cluster

    _configure_slurm

    # XXX - if we're autoscaling, then we need to add some dummy nodes
    # to represent the scaling group, and manage the addition/removal
    # of them as nodes scale-in/scale-out.

    # something like:
    # for a in {1..$group_size}; do
    #  add-node autoscaling-slot-$a $slots_in_machine_type FUTURE
    # done

    # might be better to do this on first member-join so we know what
    # kind of machine type we've got for compute?
}

setup
require handler
require files

_ALCES="${cw_ROOT}"/bin/alces

handler_tee main "$@"
