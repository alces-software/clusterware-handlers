#!/bin/bash
################################################################################
##
## Alces Clusterware - Handler hook
## Copyright (C) 2015-2016 Stephen F. Norledge and Alces Software Ltd.
##
################################################################################
setup() {
    local a xdg_config
    IFS=: read -a xdg_config <<< "${XDG_CONFIG_HOME:-$HOME/.config}:${XDG_CONFIG_DIRS:-/etc/xdg}"
    for a in "${xdg_config[@]}"; do
        if [ -e "${a}"/clusterware/config.vars.sh ]; then
            source "${a}"/clusterware/config.vars.sh
            break
        fi
    done
    if [ -z "${cw_ROOT}" ]; then
        echo "$0: unable to locate clusterware configuration"
        exit 1
    fi
    kernel_load
}

_configure_slurm() {
    local slurm_system_dirs slurm_user munge_path munge_key_dir munge_key munge_user hook_dir hostname

    echo "Configuring Slurm"

    # Create needed system dirs owned by Slurm user.
    slurm_system_dirs=(/var/{log,run,spool}/slurm)
    slurm_user='slurm'
    mkdir -p "${slurm_system_dirs[@]}"
    chown -R "${slurm_user}:${slurm_user}" "${slurm_system_dirs[@]}"

    # Create MUNGE key, required for authentication between nodes - must be the
    # same on all nodes so create by hashing the cluster secret. Note: Create
    # in this hook as need auth.rc to be created which is not done until
    # clusterable preconfigure hook.
    munge_path="$(echo ~munge)"
    munge_key_dir="${munge_path}/etc/munge"
    munge_key="${munge_key_dir}/munge.key"
    munge_user='munge'
    source "${cw_ROOT}/etc/config/cluster/auth.rc"
    echo -n "${cw_CLUSTER_auth_token}" | sha512sum | cut -d' ' -f1 > "${munge_key}"
    chmod 400 "${munge_key}"
    chown -R "${munge_user}:${munge_user}" "${munge_key_dir}"

    # Update slurm.conf and enable service components based on node's role(s).
    hook_dir=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
    hostname="$(hostname)"
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
        echo "Instance is Slurm control node"
        "${hook_dir}/share/slurm-set-control-node.sh" "${hostname}"
        "${cw_ROOT}"/bin/alces service enable slurm/slurmctld
    fi
    if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":compute:"* ]]; then
        echo "Instance is Slurm compute node"
        slurm_add_compute_node "${hostname}"
        "${cw_ROOT}"/bin/alces service enable slurm/slurmd
    fi
}

_detect_autoscaling() {
    "${_AWS}" --region "${cw_INSTANCE_aws_region}" \
              autoscaling describe-auto-scaling-groups &>/dev/null
}

_configure_aws_autoscaling() {
    local group tags hook_dir

    # determine if we're running on EC2
    if files_load_config --optional instance-aws config/cluster; then
        echo "Instance is running on EC2; configuring for autoscaling"
        files_load_config config config/cluster

        # we are; ensure aws serviceware is installed
        "${_ALCES}" service install aws

        if [ "${cw_INSTANCE_aws_autoscaling}" == "autodetect" ]; then
            if _detect_autoscaling; then
                echo "Autoscaling detected as enabled"
                sed -i -e 's/^cw_INSTANCE_aws_autoscaling=.*/cw_INSTANCE_aws_autoscaling=enabled/' \
                    "${cw_ROOT}"/etc/config/cluster/instance-aws.rc
            else
                echo "Autoscaling detected as disabled"
                sed -i -e 's/^cw_INSTANCE_aws_autoscaling=.*/cw_INSTANCE_aws_autoscaling=disabled/' \
                    "${cw_ROOT}"/etc/config/cluster/instance-aws.rc
            fi
        fi

        # install autoscale management cron job if we're a master
        if [[ "${cw_INSTANCE_tag_SCHEDULER_ROLES}" == *":master:"* ]]; then
            hook_dir=$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)
            echo "Installing autoscaler cronjob: ${hook_dir}/share/sge-autoscaler"
            echo "*/5 * * * * root ${hook_dir}/share/sge-autoscaler" > /etc/cron.d/clusterware-cluster-sge-autoscaler
        fi

        # set serf tags containing:
        #  - instance creation time
        #  - autoscaling group name (if available)
        tags=(tags[aws_ctime]=${cw_INSTANCE_aws_ctime})

        group=$("${_AWS}" --region "${cw_INSTANCE_aws_region}" \
                          autoscaling describe-auto-scaling-groups | \
                       grep AutoScalingGroupName | grep "\"${cw_CLUSTER_name}-" | \
                       awk '{print $2}' | tr -d '",')
        if [ "$group" ]; then
            tags+=(tags[aws_group]=${group})
        fi
        echo "Setting autoscaler tags: ${tags[@]}"
        "${_JO}" "${tags[@]}" > "${cw_ROOT}"/etc/serf/tags-cluster-sge-aws.json
    fi
}

main() {
    local handler_config_dir="${cw_ROOT}/etc/config/cluster-slurm"

    # Bail out if we're already configured (this is a reboot)
    if [ -d "${handler_config_dir}" ]; then
        exit 0
    fi

    mkdir -p "${handler_config_dir}"

    files_load_config instance config/cluster

    _configure_slurm
    # _configure_aws_autoscaling # TODO: Adapt autoscaling for Slurm.
}

setup
require ruby
require handler
require files

_AWS="${cw_ROOT}"/opt/aws/bin/aws
_JO="${cw_ROOT}"/opt/jo/bin/jo
_ALCES="${cw_ROOT}"/bin/alces

handler_tee main "$@"
